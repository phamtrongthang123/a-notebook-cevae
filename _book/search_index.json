[
["index.html", "A notebook Paper Causal Effect Inference with Deep Latent-Variable Models Chapter 1 Overview 1.1 Keywords 1.2 Distinguishing do() and cond():", " A notebook Paper Causal Effect Inference with Deep Latent-Variable Models Trong-Thang Pham 2020-04-12 Chapter 1 Overview This is a notebook I write about CEVAE original paper (Louizos et al. 2017). Some grammar mistakes are going to be made. The authors’ source code: https://github.com/AMLab-Amsterdam/CEVAE Here is some background knowledge that will help you to understand the whole paper. 1.1 Keywords proxy, or proxy variable: is a variable that represents for confounders, indirectly help on finding causal effect of confounders. Recognition network: is the encoder of VAE, can also be called inference model Deep Latent-Variable Models (DLVM): denote a LVM \\(P_\\theta(x,z)\\) whose distribution are parameterized by neural net Covariate: variable can predict the outcome Ignorability: outcome does not depend on missing data Observational study: They are called observational studies because the investigator observes individuals without manipulation or intervention. Randomised controlled trials (RCT), or Randomized trial: this trial require many people, then randomly split into 2 groups, investigators do intervene (give treatment in 1 group) and look at the effects of the intervention on an outcome. Within-sample: run experiment on train/val set Out-of-sample: run experiment on test set Causal inference: focus on effect of treatment on the outcome, even though we only know the result from 1 side (get treatment or not), this is factual results. In this paper, we care about produce a result from the other side, which is not real, we called it counterfactual. We do that by learning a generative model, then intervene the treatment variable and collect new results, those are our counterfactual results. Treatment group: the group get the treatment, for example doing physical exercise. Control group: the group don’t get the treatment, for example just sleep all day long. 1.2 Distinguishing do() and cond(): cond() is a notation I made up to stand for “is conditioned on”. For example, y is conditioned on x, then I call it y cond(x). Actually using y|x is more elegant but if we talking normally without the y, using | or |x is too short, in my opinion. do() is represented for an action called intervent. Intervene is used to show causality. Says we only sure A cause B if and only if (iff) A is there, B behave this, but without A, B behave differently. Counterfactual problem: In reality, each individual could only be in control or treatment group. When we talk about P(b|a), we are looking at if a=0 or a=1 in general view. We can’t be sure how it behave for each individual. That’s why we need to seek a new view to inference that individual effect. Intervene is an action change the treatment value (from treat to control, or vice versa) in order to observe how individual situation different from the origin treatment. For example: Patient x didn’t get his treatment and died (for very long time ago). What is the outcome if he get his treatment? If we base on p(b|a) to answer, then it is just indirect way. The direct way is p(b|do(a)). It’s sound pretty abstract because how can we intervene when it’ve already happened. We can’t in real life, but we can approximate that if we correcty model the causal effect of it. That’s why we have this study. Anyway there is another view from how to compute do() and cond(). We compute p(b|a) when we sample both a and b, then cond(a). do(a) is when we fix value of a in the beginning, then sample b. Those two operations can have the same value, but not in all cases. Some aspect we can only discover iff we put do() on it. References "],
["intro.html", "Chapter 2 Introduction 2.1 Abstract 2.2 Old study: 2.3 Propose", " Chapter 2 Introduction 2.1 Abstract Build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect Based on VAE, with proxies 2.2 Old study: give methods which are guaranteed to recover the true causal effect when proxies are observed. However, the strong guarantees these methods enjoy rely on strong assumptions. In particular, it is assumed that the hidden confounder is either categorical with known number of categories, or that the model is linear-Gaussian. =&gt; In practice, we cannot know the exact nature of the hidden confounder Z 2.3 Propose Approach tailored to the surrogate-rich setting when many proxies are available: Estimation of a latent-variable model where we simultaneously: discover the hidden confounders ( &lt;= old study put assumption on this, now we remove it) infer how they affect treatment and outcome In this graph, Observation are gray, hidden confounder is white. I drew it in reverse color at all graphs below. Method: based on VAE Disadvantage: little theory is currently available to justify when learning with VAEs can identify the true model =&gt; Which mean we pretty much don’t know if it is learning correctly. Advantage: they make substantially weaker assumptions about the data generating process and the structure of the hidden confounders "],
["indentify.html", "Chapter 3 Identification of causal effect 3.1 Identifying individual treatment effect", " Chapter 3 Identification of causal effect Question from the title: what is the purpose of this part? What does Identification mean? Assume: Treatment t is binary, for simplicity and compatibility with prior benchmarks (in IHDP and Jobs benchmark, treatment is yes or no). However proposed model does not limit by this (We don’t have benchmark for this so I don’t know if it is true) joint distribution p (Z, X, t, y) of the latent confounders Z and the observed confounders X can be approximately recovered solely from the observations (X, t, y) With 2nd assumption, we can’t compute if X has nothing to do with Z. But there are many cases we can. Authors said they show some example in Chapter 2. But in the introduction, we only saw example X is proxy or noise of Z. Besides, authors also list 3 cases: if X includes three independent views of Z if Z is categorical and X is a Gaussian mixture model with components determined by X if Z is comprised of binary variables and X are so-called “noisy-or” functions of Z Take the last case, Z is the input of X, so X also directly relate to Z. We could conclude that the idea is if Z-&gt;X then we can compute it. In the paper, the authors also talk about VAE could recover many Latent Variable Model, I have read the reference paper, in that paper it’s called Variational Gaussian process (VGP) with similar idea to VAE (encode input -&gt; z and then decode z -&gt; output) VGP can recover discrete mixture of mean-field distributions and recover a form of factor analysis. (I don’t know any of these at this point) Caveat: optimization process is not guaranteed to achieve the true minimum even if it is within the capacity of the model, similar to the case of classic universal approximation (UA) results for neural networks. Reading caveat remind us that the reason could be from using NN. 3.1 Identifying individual treatment effect We care about finding ITE (another name is CATE) and ATE \\[I T E(x):=\\mathbb{E}[\\mathbf{y} | \\mathbf{X}=x, d o(\\mathbf{t}=1)]-\\mathbb{E}[\\mathbf{y} | \\mathbf{X}=x, d o(\\mathbf{t}=0)]\\] \\[A T E:=\\mathbb{E}[I T E(x)]\\] Identification in our case is result of Pearl’s back-door adjustment. Theorem 1. If we recover p (Z; X; t; y) then we recover the ITE under the causal model in Figure 1. Proof. We will prove that \\(p(\\mathbf{y} | \\mathbf{X}, d o(\\mathbf{t}=1))\\) is identifiable under the premise of the theorem. The case for \\(t=0\\) is identical, and the expectations in the definition of ITE above readily recovered from the probability function. ATE is identified if ITE is identified. We have that: \\[ p(\\mathbf{y} | \\mathbf{X}, d o(\\mathbf{t}=1))=\\int_{\\mathbf{Z}} p(\\mathbf{y} | \\mathbf{X}, d o(\\mathbf{t}=1), \\mathbf{Z}) p(\\mathbf{Z} | \\mathbf{X}, d o(\\mathbf{t}=1)) d \\mathbf{Z} \\stackrel{(i)}{=} \\] \\[ \\int_{\\mathbf{Z}} p(\\mathbf{y} | \\mathbf{X}, \\mathbf{t}=1, \\mathbf{Z}) p(\\mathbf{Z} | \\mathbf{X}) d \\mathbf{Z} \\ \\ \\ \\ \\ (1) \\] where equality (i) is by the rules of do-calculus applied to the causal graph in Figure \\([1] .\\) This completes the proof since the quantities in the final expression of Eq. (1) can be identified from the distribution \\(p(\\mathbf{Z}, \\mathbf{X}, \\mathbf{t}, \\mathbf{y})\\) which we know by the Theorem’s premise. There are 2 small note: Note that the proof and the resulting estimator in Eq. (1) would be identical whether there is or there is not an edge from X to t. This is because we intervene on t \\(\\text { y is independent of } \\mathbf{X} \\text { given } \\mathbf{Z}, \\text { and we obtain: } p(\\mathbf{y} | \\mathbf{X}, d o(\\mathbf{t}=1))=\\int_{\\mathbf{Z}} p(\\mathbf{y} | \\mathbf{t}=1, \\mathbf{Z}) p(\\mathbf{Z} | \\mathbf{X}) d \\mathbf{Z}\\) Questions: What does Identification mean? In proof, the authors use it as a way to say we can compute it, or identifiable. But using in noun form is weird to me. In Pearl’s Causality page 76, the noun form of it represent a kind of question where answer if arbitrary operation Q is computable. What is Pearl’s back-door adjustment? Theorem 3.3.2 (Back-Door Adjustment) (page 79 Causality) If a set of variables Z satisfies the back-door criterion relative to (X, Y), then the causal effect of X on Y is identifiable and is given by the formula \\[P(y | \\hat{x})=\\sum_{z} P(y | x, z) P(z)\\] Is the sum equal to P(y|x)? No, except where z is independ to x. Because \\(P(y|x) = \\sum P(y|x,z)P(z|x)\\) What is the hat? it is another notation for do(x) What is back-door criterion? We called it back-door because every arrow go to \\(X_i\\) ( sentence (ii) in the definition). Pearl show (with proof) that X-&gt;Y has do(X) equal to cond(X) when Z blocks all the back-door, which mean y|do(x) have the same result as y|x) The idea of the proof is create another variable call \\(F_i\\), for example: At this time, all back-door is blocked (if cond() on the small note in the top) then every path from \\(F_i \\rightarrow Y\\) (Y is the rightest node) have to fo through \\(X_i\\)’s children. If we cond(Xi), \\(F_i\\) is independ to Y. Thus do(x) = cond(x) Why can we consider do() as a variable like that? I guess because when we use do(), we want to put an action on it, which mean there is a relationship. We can conceptualize this action. The detail of definition for \\(F_i\\) readers can read in Causality, page 71. In short, when we say we use back-door adjustment, the idea is we want to convert do() into normal cond(). At this moment, we can safely say that the intent of the authors in this part is to say that their model could be compute and infer causal inference (through do-calculus) "],
["CEVAE.html", "Chapter 4 Causal effect variational autoencoder", " Chapter 4 Causal effect variational autoencoder Question from the title: Why use VAE? Why use the propose architecture? Why use that constant and this prior? What is the loss, how define, what is the term, why is that? Let’s remind ourself about VAE: Why use VAE? The authors didn’t explain much. I guess they want to use the similar idea of finding hidden variable. And it’s more resonable when VAE (VCG) can recover many model. The architectures: Note: no matter how, Y is influenced by T. Then the authors: Assume that obs factorize conditioned on latent variable Using inference network follow a factorization of the true posterior Using generative network architecture inspired by TARnet, but instead of conditioning on observations we condition on the latent variables z True posterior ? It took me a while to understand. The idea is using chain rule \\[Q(z,y,t|x) = Q(z|x,t,y)Q(y|x,t)Q(t|x)\\] Why we choose those term? In the case \\(Q(z,y,t|x)\\), maybe it is the consequence of choosing the architecture of model, which is inspired by TARNet, z impact on x,y,t so now to compute posterior we have to condition on those. Moreover, I can see that the authors want to use all data they got to find z. Other terms are unclear to me. What is “instead of conditioning on observations we condition on the latent variables z” ? The authors trying to different the original model TARNet, in the original they only condition on x to compute y, now we use z instead (paragraph 2 part 4 paper (Shalit, Johansson, and Sontag 2016)) \\[\\begin{aligned} &amp;y_{i} \\sim p\\left(Y_{1} | x_{i}\\right) \\text { if } t_{i}=1\\\\ &amp;y_{i} \\sim p\\left(Y_{0} | x_{i}\\right) \\text { if } t_{i}=0 \\end{aligned}\\] What is TARNet? TARNet is a network using neural network to capture relationship, a feed forward network. Integral Probability Metric (IPM) measure of distance between two distributions p(x|t = 0), and p(x|t = 1) Objective: \\(\\alpha &gt; 0\\) then called CFR, \\(alpha= 0\\) is TARNet (remove the IPM term) Algorithm to learn CFR (remove IPM is TARnet). Why do we use nn in eevry place? The authors probably want using the flexible of nn to capture relationships. The define prior and density part are pretty detail, readers can read in the paper. But there are question as well. What is switching mean? It is basically the ta + (t-1)b part, meaning switch between a or b base on t status. Why use \\(N(0,1)\\) as prior to z? Can we change it? Yes, we can change, readers will see that in toy benchmark later. It is just a way to define z. Why we need two \\(g()\\) in guide, when only need 1 \\(f()\\) in model? In guide we want to use x,y to learn about z. X consist of all kind of distribution, e.g categorical, normal, binomial …, that’s why we need an neural network to learn the representation space first, then apply encode network. In model our z is just one distribution, so no need of second network. Why does the lower bound have formula like that? Note: \\(log p(x)\\) independ to q, so we consider it is a constant. If we see x as the tuple x,y,t then the \\(\\mathcal{L}\\) becomes the lower bound we saw in the paper. What does auxiliary distributions mean? We know that VAE only encode X to z, then z decode to output. In our case, our guide need addition y,t to compute z. That’s why the authors said we need to predict those two. References "],
["exp.html", "Chapter 5 Experiments 5.1 Benchmark dataset", " Chapter 5 Experiments 5.1 Benchmark dataset There are 2 benchmark people usually use is Infant Health and Development Program (IHDP) and Jobs. The authors use those so there is no need to construct proxies. But to show the power of learning proxies, the authors proposed 2 other benchmarks is synthesis data and Twins. 5.1.1 IHDP Context: Treatment group will be provide special care and home visits from a trained provider. In the original study, the final cognitive test scores (CTS) is higher when they are treated. We care about the effect of home visits by specialists on future cognitive test scores If t=1, child get treatment, t=0 is not. Description: IHDP consists of 6 continuous 19 binary covariates. Outcome is simulated Treatment and control group is unbalanced, 139 is treated, 608 in control. Outcome using in this paper is from Setting A from https://github.com/vdorie/npci (is the Setting B in (Hill 2011)). Produce 1000 replications. Split train/val/test set with the same ratio as (Shalit, Johansson, and Sontag 2016) for each replications. For each replication, the networks will be retrain and compute error and final results will be averaged. Outcome Y is simulated, distribution is Normal, nonlinear and not parallel across treatment conditions. Y is depend on X to satisfy ignorability. In my understanding, the graphical model will look like: 2 main tasks: estimating the individual / population causal effects Individual treatment effect is Precision in Estimation of Heterogeneous Effect (PEHE) (reflects the ability to capture individual variation in treatment effects). PEHE \\(=\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\left(y_{i 1}-y_{i 0}\\right)-\\left(\\hat{y}_{i 1}-\\hat{y}_{i 0}\\right)\\right)^{2},\\) where \\(y_{1}, y_{0}\\) correspond to the true outcomes under \\(t=1\\) and \\(t=0,\\) respectively, and \\(\\hat{y}_{1}, \\hat{y}_{0}\\) correspond to the outcomes estimated by our model. population causal effect: absolute error on the Average Treatment Effect (ATE) \\(\\epsilon_{\\mathrm{ATE}}=| \\frac{1}{n} \\sum_{i=1}^{n}\\left(f\\left(x_{i}, 1\\right)-f\\left(x_{i}, 0\\right)\\right)-\\frac{1}{n} \\sum_{i=1}^{n}\\left(m_{1}\\left(x_{i}\\right)-\\right.\\left.m_{0}\\left(x_{i}\\right)\\right) | .\\) with \\(m_{1}(x)=\\mathbb{E}\\left[Y_{1} | x\\right], m_{0}(x)=\\mathbb{E}\\left[Y_{0} | x\\right]\\) Those above metrics the smaller the better 5.1.2 Jobs: Pretty much the same as IHDP benchmark, except this is the real outcome from real life, not simulated. Evaluation: Use policy risk to replace ITE because we don’t know the other treatment. Thus, we don’t have groundtruth for ITE Population CE: use absolute error on the Average Treatment effect on the Treated (ATT) for the exact same reason. The authors didn’t public this dataset. 5.1.3 Synthesis toy data: Read paper for this part, it’s not much content. 5.1.4 Binary treatment outcome on Twins Context: Data from twin births in the USA between 1989-1991, only chose twins which are the same sex, focused on twins such that both were born weighing less than 2kg The treatment t = 1 is being born the heavier twin The outcome corresponds to the mortality of each of the twins in their first year of life =&gt; could be considered as the two potential outcomes for each treatment =&gt; We want to know the effect of being born heavier on the mortality in first year. Description: Dataset of 11984 pairs of twins, for each twin pair we observed both the case t = 0 (lighter twin) and t = 1 (heavier twin) 46 covariates: mother and father education, marital status, race and residence; number of previous births; pregnancy risk factors such as diabetes, renal disease, smoking and alcohol use; quality of care during pregnancy; whether the birth was at a hospital, clinic or home; and number of gestation weeks prior to birth. More detail about each covariate is in my read data Twins notebook Missing value xử lý: For the features that had more than 20% missing values we assigned to the missing values a constant number that was the maximum of the existing values + 1. Essentially this creates a separate ‘nan category’ for those features. For the rest we used a most-frequent imputation strategy. Simulate 2 kind of trial: Observational study: selectively hide one of the two twins Randomized trial: randomly hide one To simulate hidden confounding with proxies: Condition the treatment variable under highly correlated variable with outcome, the authors chose GESTAT10 (the define in paper is unclear to me why they choose w0,w1 that way, and the GESTAT10 itself is complicated too) To create proxies, the authors encode G10 1hot, replicate 3 times (unclear these are in the same data or split to retrain like IHDP). The reason authors choose 3 replicate because the references in the paper show that only 3 replicate is enough the recover the origin encoded variable. With each bit in 30bits above, randomly and independently flipped with prob varies from 0.05 to 0.5, latter indicating there is no direct information about the confounder. Note: In the public data, the authors give the raw data, haven’t encode G10 into 1hot. I assume 3 replicate is in the same data. I need to read more into reference papers to understand which one is correct. The graphical model will be like: Groundtruth: mortality rate for the lighter twin (t=0) is 18.9%, and for the heavier (t=1) 16.4%, for an average treatment effect (ATE) of -2.5% 2 tasks inferring: the mortality of the unobserved twin (counterfactual): Predict alive or death, then get accuracy, then draw AUC as below pictures \\(\\epsilon_{A T E}=|A \\hat{T} E-A T E |\\), with \\(A \\hat{T} E\\) is the ratio of outcome t=1 and t=0 substract eachother. (ratio of alive cases) In comparision to other methods, in Figure (a), the higher the better, and (b) the lower the better. References "],
["references.html", "References", " References "]
]
